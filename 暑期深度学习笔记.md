# 一、神经网络与深度学习

## 1 深度学习概论

### 1.1 欢迎来到深度学习
- AI的意义
- 本门课程将要学到的内容

### 1.2 神经网络简介
常用模型激活函数：ReLU函数
神经网络结构：输入 -> 中间层 -> 输出

### 1.3 用神经网络进行监督学习
- 各AI方面算法规模与性能的比较
- 神经网络发展三要素：数据、计算力、算法
- 研究过程：Idea -> Code -> Experiment -> Idea -> ...

## 2 神经网络基础

### 2.1 二分类
对样本特征进行分析，最终将其分为两类的一类型问题。

### 2.2 Logistics Regression
给定样本x，求$\hat{y}=P(y=1|x)$
- 输入:$x,长度为n_x$的特征向量
- 输出:$\hat{y}$,回归估计值,介于0与1之间
- 参数:$w,b$
- 激活函数:sigmoid函数($\sigma(z)=\frac{1}{1+e^{-z}}$)

$$\hat{y}=\sigma(z)$$
$$z=wx+b$$
$$\sigma(z)=\frac{1}{1+e^{-z}}$$

### 2.3 梯度下降法的损失函数
**损失函数**:
$$\mathcal L(\hat{y},y)=-(y\log{\hat{y}}+(1-y)\log{(1-\hat{y})})$$

**代价函数**:
$$J(w,b)=\frac{1}{m}\sum^m_{i=1}{L(\hat{y},y)}=-\frac{1}{m}\sum^m_{i=1}{y^{(i)}\log{\hat{y}^{(i)}}+(1-y^{(i)})\log{(1-\hat{y}^{(i)})}}$$

### 2.4 梯度下降法
要找的参数$(w,b)$使得代价函数$J(w,b)$取得最小值,得到这组参数的方法可以使用梯度下降法.梯度下降法可以使用如下公式表示:
$$
w = w - \alpha \frac{\partial J}{\partial w}
$$
$$
b = b -  \alpha \frac{\partial J}{\partial b}
$$

### 2.5 导数

### 2.6 更多导数的栗子

### 2.7 计算图 
用流程图的方式来表示整个计算过程,方便理解正向计算与反向计算.在图中的求导主要利用了链式求导法则.

### 2.8 计算图中的导数计算
链式求导

### 2.9 Logistic回归中的梯度下降
Logistic回归正向计算过程
$$z = w^Tx + b$$
$$\hat{y} = a = \sigma (z)$$
$$\mathcal L(a, y) = -(y\log(a) + (1 - y)\log(1 - a))$$
反向计算(在变量名前加d表示最终结果函数对其求导):
$$da = \frac{\partial \mathcal{L}}{{\partial a} = \frac{1 - y}{1 - a} - \frac{y}{a}$$
$$dz = a - y$$
$$dw_1 = x_1dz$$
$$dw_2 = x_2dz$$
$$db = dz$$